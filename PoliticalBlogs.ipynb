{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from scipy import sparse\n",
    "from random import shuffle\n",
    "\n",
    "from numpy import linalg\n",
    "from networkx.algorithms.community.label_propagation import label_propagation_communities\n",
    "import collections\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "political=nx.read_gml(\"polblogs.gml\",label='id')\n",
    "\n",
    "#make the graph directed with no parallels\n",
    "political = nx.DiGraph(political)\n",
    "\n",
    "#returns a set of nodes of the biggest connected component\n",
    "largest = max(nx.connected_components(political.to_undirected()), key=len)\n",
    "\n",
    "# creates the graph of the biggest connected component including direction\n",
    "CC_max = political.subgraph(largest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "9.223372036854776e+18\n"
     ]
    }
   ],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "a = CC_max.copy()\n",
    "\n",
    "convergence_factor = 0.01\n",
    "\n",
    "#get the b% of values\n",
    "percentage=0.1\n",
    "\n",
    "# re-index node id's to simply proccess, original id's are stored in a label.\n",
    "CC_max = nx.convert_node_labels_to_integers(CC_max, first_label=0, ordering='default', label_attribute='original_graph_id')\n",
    "\n",
    "#Returns a list with the value of the nodes\n",
    "original_values = dict(CC_max.nodes(data='value'))\n",
    "\n",
    "numbers_of_items_kept_from_percentage = round(len(a)*percentage)\n",
    "\n",
    "# shuffle the dictionary of values to get the random nodes\n",
    "\n",
    "shuffled = dict(original_values.items())\n",
    "\n",
    "\n",
    "shuffled_list = list(shuffled.items())\n",
    "np.random.shuffle(shuffled_list)\n",
    "shuffled = dict(shuffled_list)\n",
    "\n",
    "\n",
    "#keep the first percentage b% of the nodes\n",
    "nodes_kept = list(shuffled.items())[:numbers_of_items_kept_from_percentage]\n",
    "\n",
    "nodes_kept_dictionary = dict(nodes_kept)\n",
    "\n",
    "nodes_to_discover = list(shuffled.items())[numbers_of_items_kept_from_percentage:]\n",
    "\n",
    "nodes_to_be_discovered_dictionary = dict(nodes_to_discover)\n",
    "\n",
    "nodes_with_known_value = list(dict(nodes_kept).keys())\n",
    "\n",
    "\n",
    "#initialize not known node values\n",
    "        \n",
    "for key in dict(nodes_kept).keys():\n",
    "    shuffled[key] = 0.5\n",
    "\n",
    "\n",
    "#the previous values holds a n x 2 matrix with the original node values\n",
    "#the first column represent the id the second one the node value\n",
    "previous_values = np.matrix(list(original_values.items()))\n",
    "\n",
    "#drop id's so we have a clean n x 1 matrix with the original values\n",
    "previous_values = np.delete(previous_values, 0, 1)\n",
    "\n",
    "#the same applies for the new values variable\n",
    "new_values = np.matrix(list(original_values.items()))\n",
    "\n",
    "#drop id's\n",
    "new_values = np.delete(new_values, 0, 1)\n",
    "\n",
    "#the previous_values and new_values will be updated inside the while loop\n",
    "\n",
    "# adjacency matrix of the graph\n",
    "A = nx.adjacency_matrix(CC_max)\n",
    "\n",
    "# a n x 1 matrix that holds how many neighbors each node has\n",
    "sum_of_neighbors = np.sum(A,axis=1)\n",
    "\n",
    "# numbers of nodes\n",
    "size_of_vector = np.size(sum_of_neighbors,0)\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "    new_values = A * new_values\n",
    "    \n",
    "    #new_values after the multiplication is a n x 1 matrix\n",
    "    #we need to change the known nodes to their original value\n",
    "\n",
    "    for key in nodes_kept_dictionary.keys():\n",
    "        new_values[key] = nodes_kept_dictionary[key]\n",
    "\n",
    "    #no we need to divide all nodes that we are trying to predict with how many nieghbors they have\n",
    "    \n",
    "    for key in nodes_to_be_discovered_dictionary.keys():\n",
    "        new_values[key] = new_values[key] / sum_of_neighbors[key]\n",
    "        \n",
    "    # 1 x n Matrix\n",
    "    reshape_previous_for_norm = np.array(np.reshape(previous_values,(1, size_of_vector)))\n",
    "    \n",
    "    norm_previous = linalg.norm(reshape_previous_for_norm,1)\n",
    "    \n",
    "    # 1 x n Matrix\n",
    "    reshape_next_for_norm = np.array(np.reshape(new_values,(1, size_of_vector)))\n",
    "    \n",
    "    norm_new = linalg.norm(reshape_next_for_norm,1)\n",
    "    \n",
    "    print(norm_previous)\n",
    "    print(norm_new)\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#norm_difference = abs(norm_new - norm_of_previous_values)\n",
    "\n",
    "#print(norm_difference)\n",
    "\n",
    "#if norm_difference < convergence_factor:\n",
    "#    break\n",
    "\n",
    "#previous_values = new_values         \n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
